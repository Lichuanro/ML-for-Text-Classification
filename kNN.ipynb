{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN for Text Classification\n",
    "kNN classification assigns the majority class of the k nearest neighbors to a test document.   \n",
    "  \n",
    "kNN requires no explicit training and can use the unprocessed training set directly in classification. It's also called “Lazy learning”  \n",
    "  \n",
    "When k is set to 1, it's called 1NN and it's not very robust intuitively. Because the decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical. kNN for k > 1 is more robust. It assigns documents to the majority class of their k closest neighbors, with ties broken randomly.  \n",
    "  \n",
    "Since the algorithm relies on distance between samples, we need a way to define our distance.  \n",
    "  \n",
    "The most popular distance measure is Euclidean distance. It's like the length of the straight line between two points.  \n",
    "  \n",
    "Cosine similarity is another popular measurement of similarity. Suppose $x$ and $y$ are two vectors, the cosine distance is defined:  \n",
    "  \n",
    "$\\frac{x \\cdot y}{||x|| ||y||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "The data is in the format of **label word:count word:count ...**  \n",
    "For example, \"talk.politics.guns a:4 accidents:2 advance:1 age:2 an:1 and:3 any:1\"  \n",
    "\n",
    "To load the data, simply split every line by space, take the first item as label and split with colon for the rest items to get word and corresponding count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Parse one line in the file\n",
    "    return: set of words, class\n",
    "    \"\"\"\n",
    "    l = line.strip().split()\n",
    "    tag = l[0]\n",
    "    word_set = set([w.split(':')[0] for w in l[1:]])\n",
    "    return word_set, tag\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\"\n",
    "    Parse file\n",
    "    return: X - word set list, y - class list\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            word_set, target = parse_line(line)\n",
    "            X.append(word_set)\n",
    "            y.append(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build kNN class\n",
    "\n",
    "The key part in kNN is caluating the distance in the vector space, here I find scipy useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "\n",
    "class kNN:\n",
    "    def __init__(self):\n",
    "        self.X_raw = None\n",
    "        self.y_raw = None\n",
    "        self.y = None\n",
    "        self.idx_word = None\n",
    "        self.idx_label = None\n",
    "        self.word_idx = None\n",
    "        self.label_idx = None\n",
    "        self.m = None\n",
    "\n",
    "    def __create_feature_class_matrix__(self, X, y):\n",
    "        bow = set(chain.from_iterable(X))\n",
    "        self.X_raw = X\n",
    "        self.y_raw = y\n",
    "        self.idx_word = {i: w for i, w in enumerate(sorted(bow))}\n",
    "        self.word_idx = {v: k for k, v in self.idx_word.items()}\n",
    "        self.idx_label = {i: l for i, l in enumerate(sorted(set(y)))}\n",
    "        self.label_idx = {v: k for k, v in self.idx_label.items()}\n",
    "        m = np.zeros([len(X), len(self.idx_word)])\n",
    "        for idx, X_ in enumerate(X):\n",
    "            for w in X_:\n",
    "                m[idx][self.word_idx[w]] += 1\n",
    "        self.m = m\n",
    "        self.y = np.array([self.label_idx[i] for i in y])\n",
    "\n",
    "    def __calculate_distance__(self, inst, dist='euclidean'):\n",
    "        if dist == 'euclidean':\n",
    "            d_m = cdist(inst, self.m)\n",
    "        elif dist == 'cosine':\n",
    "            d_m = cdist(inst, self.m, metric='cosine')\n",
    "        return d_m\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.__create_feature_class_matrix__(X, y)\n",
    "\n",
    "    def __predict__(self, X, k, dist):\n",
    "        m_pre = np.zeros([len(X), len(self.idx_word)])\n",
    "        for idx, X_ in enumerate(X):\n",
    "            for w in X_:\n",
    "                pos = self.word_idx.get(w)\n",
    "                if pos:\n",
    "                    m_pre[idx][pos] += 1\n",
    "        d_m = self.__calculate_distance__(m_pre, dist)\n",
    "        top_k = d_m.argsort()[:,:k]\n",
    "        pre = []\n",
    "        for i in range(len(top_k)):\n",
    "            label = Counter(self.y[top_k[i]]).most_common()[0][0]\n",
    "            pre.append(label)\n",
    "        return np.array(pre)\n",
    "\n",
    "    def predict(self, X, k=5, dist='cosine'):\n",
    "        pre_idx = self.__predict__(X, k, dist)\n",
    "        return [self.idx_label[i] for i in pre_idx]\n",
    "\n",
    "    def evaluate(self, X, y, k=5, dist='cosine'):\n",
    "        pre = self.__predict__(X, k, dist)\n",
    "        true = np.array([self.label_idx.get(i, len(self.label_idx)) for i in y])\n",
    "        return np.sum(pre==true) / len(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'data/train.txt'\n",
    "test_data = 'data/test.txt'\n",
    "\n",
    "X_train, y_train = parse_file(training_data)\n",
    "X_test, y_test = parse_file(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = kNN()\n",
    "knn.train(X_train, y_train)\n",
    "knn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default k=5 and cosine distance measurement, we got a high accuracy, but it's not as good as Naive Bayes. However we cannot say Naive Bayes is definitely better than kNN, since there\tis a tradeoff between bias and variance, kNN has high variance and low bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
