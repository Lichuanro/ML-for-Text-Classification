{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitnlpbookcondad4249c7ccc4040c9bfb0ad7a6e926f59",
   "display_name": "Python 3.8.1 64-bit ('nlpbook': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Text Classification\n",
    "\n",
    "Multilayer Perceptron is the most traditional types of deep learning, the architecture is simple: every node in previous layer is connected to every node in the next layer.\n",
    "\n",
    "Each perceptron unit has an input (x), an output (y), and three “knobs”: a set of weights (w), a bias (b), and an activation function (f). The weights and the bias are learned from the data, and the activation function is handpicked depending on the network designer’s intuition of the network and its target outputs. Mathematically, we can express this as follows:\n",
    "\n",
    "y = f ( wx + b )\n",
    "\n",
    "Essentially, a perceptron is a composition of a linear and a nonlinear function. The linear expression wx+b is also known as an affine transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): the size of the input vectors\n",
    "            hidden_dim (int): the output size of the first Linear layer\n",
    "            output_dim (int): the output size of the second Linear layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the MLP\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Parse one line in the file\n",
    "    return: set of words, class\n",
    "    \"\"\"\n",
    "    l = line.strip().split()\n",
    "    tag = l[0]\n",
    "    word_set = set([w.split(':')[0] for w in l[1:]])\n",
    "    return word_set, tag\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\"\n",
    "    Parse file\n",
    "    return: X - word set list, y - class list\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            word_set, target = parse_line(line)\n",
    "            X.append(word_set)\n",
    "            y.append(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'data/train.txt'\n",
    "test_data = 'data/test.txt'\n",
    "\n",
    "X_train, y_train = parse_file(training_data)\n",
    "X_test, y_test = parse_file(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set.union(*X_train)\n",
    "vocab.add('<UNK>')\n",
    "id_to_token = {idx: token for idx, token in enumerate(sorted(vocab))}\n",
    "token_to_id = {token: idx for idx, token in id_to_token.items()}\n",
    "assert len(vocab) == len(id_to_token) == len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {idx: label for idx, label in enumerate(sorted(set(y_train)))}\n",
    "label_to_id = {label: idx for idx, label in id_to_label.items()}\n",
    "assert len(id_to_label) == len(label_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(convert_dict, X):\n",
    "    one_hot_matrix_size = (len(X), len(convert_dict))\n",
    "    one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "    for position_index, news in enumerate(X):\n",
    "        for word in news:\n",
    "            word_index = convert_dict.get(word, convert_dict['<UNK>'])\n",
    "            one_hot_matrix[position_index][word_index] = 1\n",
    "        \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = vectorize(token_to_id, X_train)\n",
    "X_test_onehot = vectorize(token_to_id, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = [label_to_id[y] for y in y_train]\n",
    "y_test_cat = [label_to_id[y] for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'news': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = NewsDataset(X_train_onehot, y_train_cat)\n",
    "news_dataloader = DataLoader(news_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([32, 32847])\ntorch.Size([32])\n"
    }
   ],
   "source": [
    "for d in news_dataloader:\n",
    "    print(d['news'].shape)\n",
    "    print(d['label'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MultilayerPerceptron(\n  (fc1): Linear(in_features=32847, out_features=500, bias=True)\n  (fc2): Linear(in_features=500, out_features=3, bias=True)\n)\n"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "input_dim = len(vocab)\n",
    "hidden_dim = 500\n",
    "output_dim = 3\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    for i, data in enumerate(news_dataloader):\n",
    "        news = data['news']\n",
    "        label = data['label']\n",
    "        # Step 1: Clear the gradients\n",
    "        mlp.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        output = mlp(news, apply_softmax=False)\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = criterion(output, label)\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "avg loss tensor(0.0034)\naccuracy 0.8733333333333333\n"
    }
   ],
   "source": [
    "loss = 0\n",
    "acc = 0\n",
    "news_dataset_eval = NewsDataset(X_test_onehot, y_test_cat)\n",
    "news_dataloader_eval = DataLoader(news_dataset_eval, batch_size=32, shuffle=True)\n",
    "for data in news_dataloader_eval:\n",
    "    news = data['news']\n",
    "    label = data['label']\n",
    "    with torch.no_grad():\n",
    "        output = mlp(news, apply_softmax=False)\n",
    "        loss = criterion(output, label)\n",
    "        loss += loss.item()\n",
    "        acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "print('avg loss', loss / len(news_dataset_eval))\n",
    "print('accuracy', acc / len(news_dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}