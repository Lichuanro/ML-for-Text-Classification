{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "## Features of decision tree\n",
    "* A tree with internal nodes labeled by terms\n",
    "* Branches are labeled by tests on the weight that the term has (or just presence/absence)\n",
    "* Leaves are labeled by categories\n",
    "* Classifier categorizes document by descending tree following tests to leaf\n",
    "* The label of the leaf node is then assigned to the document\n",
    "* Most decision trees are binary trees (never disadvantageous; may require extra internal nodes)\n",
    "\n",
    "## Training Process\n",
    "* Learn a sequence of tests on features, typically using top-down, greedy search\n",
    "* At each stage choose unused feature with highest Information Gain\n",
    "* At leaves, either categorical (yes/no) or continuous decisions\n",
    "\n",
    "### Find best feature for node\n",
    "Recursively choosing a best split feature at each node. A good features splits the examples into subsets that are\n",
    "ideally all positive or all negative. So we use information gain as a measurement to find the best feature.\n",
    "\n",
    "Entropy is defined at each node:\n",
    "* $p_i$ be the fraction of examples in class i\n",
    "* $p_i^f$ be the fraction of elements with feature f that lie in class i\n",
    "* $p_i^{\\neg f}$ be the fraction of elements without feature f that lie in class i\n",
    "* let $p^f$ and $p^{\\neg f}$ be the fraction of nodes with (respectively without) feature f\n",
    "\n",
    "<img src=\"./img/info_gain.jpeg\" width=\"550\">\n",
    "\n",
    "At each node, we choose the feature f which maximizes the information gain.\n",
    "\n",
    "This tends to be produce mixtures of classes at each node that are more and more “pure” as you go down the tree.\n",
    "\n",
    "If a node has examples all of one class c, we make it a leaf and output “c”. Otherwise, we potentially continue to build.\n",
    "\n",
    "If a leaf still has a mixed distribution, we output the most popular class at that node.\n",
    "\n",
    "If the features are numeric, we could discretize into bins:\n",
    "* Divide all numeric values into k bins.\n",
    "* Feature is treated as if categorical\n",
    "* Binning can be based on statistics of the entire dataset\n",
    "* For instance one might use k-means clustering on values of feature\n",
    "\n",
    "The best thing about decision tree is thatdDecision trees are easily interpreted by humans – much more easily than methods like Naive Bayes, in fact you can extract rules from decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple implementation of decision tree for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate entropy\n",
    "def get_entropy(c):\n",
    "    e = 0\n",
    "    s = sum(c.values())\n",
    "    for l in c.keys():\n",
    "        p = c[l] / s\n",
    "        e -= p*math.log2(p)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class Node():\n",
    "    \"\"\"\n",
    "    Node class for Decision Tree\n",
    "    find splits with information gain and recursive add nodes to leaf node\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, features, depth, max_depth, min_gain):\n",
    "        self.best_feature = None\n",
    "        self.available_feature = features\n",
    "        self.depth = depth+1\n",
    "        self.pos = None\n",
    "        self.neg = None\n",
    "        self.is_leaf = False\n",
    "        self.label = None\n",
    "        self.data = y\n",
    "        self.split(X, y, min_gain, max_depth)\n",
    "        rest_feature = self.available_feature - set([self.best_feature])\n",
    "        if rest_feature:\n",
    "            if not self.is_leaf:\n",
    "                X_pos, X_neg, y_pos, y_neg = [], [], [], []\n",
    "                for idx in range(len(X)):\n",
    "                    if self.best_feature in X[idx]:\n",
    "                        X_pos.append(X[idx])\n",
    "                        y_pos.append(y[idx])\n",
    "                    else:\n",
    "                        X_neg.append(X[idx])\n",
    "                        y_neg.append(y[idx])\n",
    "                self.pos = Node(X_pos, \n",
    "                                y_pos,\n",
    "                                rest_feature,\n",
    "                                self.depth,\n",
    "                                max_depth, min_gain)\n",
    "                self.neg = Node(X_neg, \n",
    "                                y_neg,\n",
    "                                rest_feature,\n",
    "                                self.depth,\n",
    "                                max_depth, min_gain)\n",
    "\n",
    "    def split(self, X, y, min_gain, max_depth):\n",
    "        class_cnt = Counter(y)\n",
    "        if self.depth <= max_depth:\n",
    "            most_gain = float('-inf')\n",
    "            feature_class_cnt = defaultdict(Counter)\n",
    "            for item in zip(X, y):\n",
    "                data, label = item\n",
    "                for feature in data:\n",
    "                    feature_class_cnt[feature].update([label])\n",
    "            for feature in self.available_feature:\n",
    "                if feature_class_cnt[feature]:\n",
    "                    # this feature has data\n",
    "                    pos = feature_class_cnt[feature]\n",
    "                    neg = class_cnt-feature_class_cnt[feature]\n",
    "                    if not neg:\n",
    "                        continue\n",
    "                    e1 = sum(pos.values())*get_entropy(pos) / sum(class_cnt.values())\n",
    "                    e2 = sum(neg.values())*get_entropy(neg) / sum(class_cnt.values())\n",
    "                    gain = -(e1+e2)\n",
    "                    if gain > most_gain:\n",
    "                        most_gain = gain\n",
    "                        self.best_feature = feature\n",
    "        if self.depth > max_depth or get_entropy(class_cnt) + most_gain < min_gain:\n",
    "            self.is_leaf = True\n",
    "            self.label = class_cnt.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Class for Decision Tree\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth, min_gain):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "\n",
    "    def train(self, X, y):\n",
    "        bow = set.union(*X)\n",
    "        self.root = Node(X, y, bow, 0, self.max_depth, self.min_gain)\n",
    "\n",
    "    def get_predict(self, x):\n",
    "        node = self.root\n",
    "        while node:\n",
    "            if node.is_leaf:\n",
    "                return node.label, node.data\n",
    "            if node.best_feature in x:\n",
    "                node = node.pos\n",
    "            else:\n",
    "                node = node.neg\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.root:\n",
    "            return\n",
    "        pre = []\n",
    "        for x in X:\n",
    "            label, data = self.get_predict(x)\n",
    "            pre.append(label)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Parse one line in the file\n",
    "    return: set of words, class\n",
    "    \"\"\"\n",
    "    l = line.strip().split()\n",
    "    tag = l[0]\n",
    "    word_set = set([w.split(':')[0] for w in l[1:]])\n",
    "    return word_set, tag\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\"\n",
    "    Parse file\n",
    "    return: X - word set list, y - class list\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            word_set, target = parse_line(line)\n",
    "            X.append(word_set)\n",
    "            y.append(target)\n",
    "    return X, y\n",
    "\n",
    "training_data = 'data/train.txt'\n",
    "test_data = 'data/test.txt'\n",
    "\n",
    "X_train, y_train = parse_file(training_data)\n",
    "X_test, y_test = parse_file(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and get some evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(10, 0.01)\n",
    "dt.train(X_train, y_train)\n",
    "y_pre = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6133333333333333\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for item in zip(y_test, y_pre):\n",
    "    correct += item[0] == item[1]\n",
    "print(correct / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
