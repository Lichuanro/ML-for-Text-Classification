{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Text Classification\n",
    "  \n",
    "## Math behind it\n",
    "Bayes rule:\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$  \n",
    "  \n",
    "In multinomial Naive Bayes, the probability of a document d being in class c is computed as  \n",
    "  \n",
    "$P(c|d) \\varpropto P(c) \\prod_{1 \\le k \\le n_d}P(t_k|c) $  \n",
    "  \n",
    "where $P(t_k|c)$ is the conditional probability of term $t_k$ occurring in a document of class $c$  \n",
    "  \n",
    "In text classiﬁcation, our goal is to find the best class for the document. The best class in NB classification is the most likely or maximum a posteriori (MAP) class $c_{map}$ :\n",
    "  \n",
    "$c_{map} = argmax P(c|d) = argmax P(c) \\prod_{1 \\le k \\le n_d}P(t_k|c)$\n",
    "  \n",
    "Since the production of probabilities may result in a ﬂoating point underflow, it would be better to use logarithm function to convert the production to sum.\n",
    "\n",
    "Given the training data, we can estimate the prior probability of each class by a simple count:  \n",
    "  \n",
    "$P(c) = \\frac{N_c}{N}$  \n",
    "where $N_c$ is the number of documents in the class $c$, and $N$ is the total number of documents\n",
    "\n",
    "The conditional probability $P(t | c)$ can be estimated by the relative frequency of term $t$ in documents belonging to class $c$:  \n",
    "  \n",
    "$P(t|c) = \\frac{T_{ct}}{T_c}$  \n",
    "where $T_{ct}$ is the number of occurrences of $t$ in training documents from class $c$ and $T_c$ is the total occurrences of words in training documents from class $c$\n",
    "\n",
    "## Why Naive Bayes?\n",
    "* Very fast learning and testing (basically just count words)\n",
    "* Low storage requirements\n",
    "* Very good in domains with many equally important features\n",
    "* More robust to irrelevant features than many learning methods\n",
    "* More robust to concept drift (changing class definition over time)\n",
    "* A good dependable baseline for text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes for text classification\n",
    "\n",
    "For the rest part, I will use Python and Numpy to implement a simple Naive Bayes Classifier for text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "The data is in the format of **label word:count word:count ...**  \n",
    "For example, \"talk.politics.guns a:4 accidents:2 advance:1 age:2 an:1 and:3 any:1\"  \n",
    "\n",
    "To load the data, simply split every line by space, take the first item as label and split with colon for the rest items to get word and corresponding count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Parse one line in the file\n",
    "    return: set of words, class\n",
    "    \"\"\"\n",
    "    l = line.strip().split()\n",
    "    tag = l[0]\n",
    "    word_set = set([w.split(':')[0] for w in l[1:]])\n",
    "    return word_set, tag\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\"\n",
    "    Parse file\n",
    "    return: X - word set list, y - class list\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            word_set, target = parse_line(line)\n",
    "            X.append(word_set)\n",
    "            y.append(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Naive Bayes class\n",
    "The input to the model is raw words list with its label, the dictionary building up is during the training process.  \n",
    "  \n",
    "The key part is calculating the probabilities, which is easy to implement with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.special import exp10\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "class MBNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.idx_word = None\n",
    "        self.idx_label = None\n",
    "        self.word_idx = None\n",
    "        self.label_idx = None\n",
    "        self.p_label = None\n",
    "        self.m_org = None\n",
    "        self.m_train = None\n",
    "\n",
    "    def __create_feature_class_matrix__(self, X, y):\n",
    "        bow = set.union(*X)\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.idx_word = {i: w for i, w in enumerate(sorted(bow))}\n",
    "        self.word_idx = {v: k for k, v in self.idx_word.items()}\n",
    "        self.idx_label = {i: l for i, l in enumerate(sorted(set(y)))}\n",
    "        self.label_idx = {v: k for k, v in self.idx_label.items()}\n",
    "        m = np.zeros([len(self.idx_label), len(self.idx_word)])\n",
    "        for X_, y_ in zip(X, y):\n",
    "            for w in X_:\n",
    "                m[self.label_idx[y_]][self.word_idx[w]] += 1\n",
    "        self.m_org = m\n",
    "\n",
    "    def train(self, X, y, class_prior_delta=0, cond_prob_delta=0.1):\n",
    "        \"\"\"\n",
    "        Training Naive Bayes\n",
    "        X: training feature (list of word set)\n",
    "        y: training label (list of class)\n",
    "        class_prior_delta: smoothing parameter for prior class probability\n",
    "        cond_prob_delta: smoothing parameter for conditional probability\n",
    "        \"\"\"\n",
    "        self.__create_feature_class_matrix__(X, y)\n",
    "        label_cnt = Counter(y)\n",
    "        p_label = np.zeros([len(label_cnt.keys())])\n",
    "        for k, v in label_cnt.items():\n",
    "            p_label[self.label_idx[k]] = v\n",
    "        p_label += class_prior_delta\n",
    "        self.p_label = p_label / p_label.sum()\n",
    "        m = self.m_org + cond_prob_delta\n",
    "        m = m / m.sum(axis=1, keepdims=True)\n",
    "        self.m_train = m\n",
    "\n",
    "    def __predict__(self, X):\n",
    "        prob = np.array([np.log10(self.p_label) for _ in range(len(X))])\n",
    "        m_pre = np.zeros([len(X), len(self.idx_word)])\n",
    "        for idx, X_ in enumerate(X):\n",
    "            for w in X_:\n",
    "                pos = self.word_idx.get(w)\n",
    "                if pos:\n",
    "                    m_pre[idx][pos] = 1\n",
    "        pre = m_pre.dot(np.log10(self.m_train).T)\n",
    "        return pre\n",
    "\n",
    "    def predict(self, X):\n",
    "        pre = self.__predict__(X)\n",
    "        return [self.idx_label[i] for i in pre.argmax(axis=1)]\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        pre = self.__predict__(X).argmax(axis=1)\n",
    "        true = np.array([self.label_idx.get(i, len(self.label_idx)) for i in y])\n",
    "        return np.sum(pre==true) / len(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NB model\n",
    "\n",
    "It's time to try the model and get some evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'data/train.txt'\n",
    "test_data = 'data/test.txt'\n",
    "\n",
    "X_train, y_train = parse_file(training_data)\n",
    "X_test, y_test = parse_file(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MBNaiveBayes()\n",
    "nb.train(X_train, y_train)\n",
    "nb.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is pretty good with such a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
